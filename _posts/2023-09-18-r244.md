---
title: An analysis of multi-GPU training performance in GNN libraries
date: 2023-09-19 14:33:00 +0000
# categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [master's, gnns, gpus, pyg, dgl]     # TAG names should always be lowercase
---

[Download coursework PDF]({{site.url}}/assets/R244_project.pdf) \| [Source code on Github](https://github.com/semiluna/R244)

---
This is yet another final coursework for one of the [5 research modules](https://www.cl.cam.ac.uk/teaching/2223/R244/) I took during my Master's year at the University of Cambridge. 

Graph neural networks (GNNs) are becoming increasingly popular, with successful applications ranging from recommender systems to protein prediction for drug design. 

Due to the intrinsically unstructured nature of graphs, GNNs are not easily parallelised, since adjacency matrices can be highly sparse and the number of neighbours of a node can vary drastically within one graph. Efforts to accelerate GNN training on GPUs have been made by two PyTorch libraries that provide deep learning tools for irregularly structured data: [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) and [Deep Graph Library](https://www.dgl.ai). 

PyTorch Geometric uses a _scatter-gather_ abstraction for accelerating computation, DGL implements sampled dense-dense and sampled sparse-dense matrix multiplication primitives in order to provide node and edge parallelism. The reported speedups of these frameworks are limited to CPU or single-GPU experiments, in this project I investigate the scalability of the two frameworks to **multi-GPU** training. 

Results indicate that the primary bottleneck in both libraries is data loading; attempts to parallelise graph sampling and loading end up in deadlocks when using more than 2 GPUs, suggesting that distributed training support is still limited.