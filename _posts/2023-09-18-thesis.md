---
title: Structure-informed protein engineering using equivariant neural networks
date: 2023-09-18 14:33:00 +0000
# categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [master's, icml, molecular biology, computational biology, geometric deep learning]     # TAG names should always be lowercase
---
[Download the full-length dissertation]({{ site.url }}/assets/aib36-project.pdf) \| [Download 4-page paper](https://arxiv.org/pdf/2306.12231.pdf) \| [Source code on Github](https://github.com/semiluna/partIII-amino-acid-prediction)

---
I wrote my Master's thesis under the supervision of [Prof Pietro Lio](https://www.cl.cam.ac.uk/~pl219) and two of his PhD students, [Simon V. Mathis](https://www.cst.cam.ac.uk/people/svm34) and [Charlie Harris](https://cch1999.github.io). 

In short, I pre-train two equivariant **graph** neural networks ([the GVP](https://arxiv.org/abs/2106.03843) and [the EQGAT](https://openreview.net/forum?id=kv4xUo5Pu6)) on the task of amino-acid residue identity prediction, and then use them as unsupervised models for **single-point mutation generation** in 49 macromolecules. 

The **key takeaway** is that structural models based on equivariant graph neural networks (EGNNs) perform **just as well** as SOTA sequence-based transformer models when predicting mutations that are better than the wildtype.[^1] However,  EGNNs are trained on ~20k protein structures, while sequence-based transformer models are trained on over ~4 million source sequences.[^2] _This could be an indication that learning from structure is much more efficient than learning from sequence when it comes to molecular tasks._

A short paper based on this work was accepted to the [ICML 2023 Computational Biology workshop](https://icml-compbio.github.io). The final version of the paper can be found on [arxiv](https://arxiv.org/abs/2306.12231). 

## ICML Paper abstract

> _Pre-trained models have been successful in many protein engineering tasks. Most notably, sequence-based models have achieved state-of-the-art performance on protein fitness prediction while structure-based models have been used experimentally to develop proteins with enhanced functions. However, there is a research gap in comparing structure- and sequence-based methods for predicting protein variants that are better than the wildtype protein. This paper aims to address this gap by conducting a comparative study between the abilities of equivariant graph neural networks (EGNNs) and sequence-based approaches to identify promising amino-acid mutations. The results show that our proposed structural approach achieves a competitive performance to sequence-based methods while being trained on significantly fewer molecules. Additionally, we find that combining assay labelled data with structure pre-trained models yields similar trends as with sequence pre-trained models. 
Our code and trained models can be found at: [this URL](https://github.com/semiluna/partIII-amino-acid-prediction)._


## ICML poster

![icml-poster]({{site.url}}/assets/ICML_poster.png)
_The poster I presented at the ICML 2023 Computational Biology workshop._

---

[^1]: By "better than the wildtype", I mean that the mutated protein's "fitness" is better than the original protein. "Fitness" also means different measures in different contexts: for some proteins, fitness is measured as thermostability. For viruses, fitness is measured as infectivity. The fitness referred to in this project is the one used in the [ProteinGym dataset](https://arxiv.org/abs/2205.13760).

[^2]: The SOTA sequence model I am comparing against is the [Tranception model](https://arxiv.org/abs/2205.13760), trained on the [UniRef100 database](https://pubmed.ncbi.nlm.nih.gov/25398609).
