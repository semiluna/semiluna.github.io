---
title: Pruned teachers are friendly teachers - improving knowledge distillation in GAT networks
date: 2023-09-19 14:33:00 +0000
# categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [master's, gnns, pruning, kd, knowledge distillation]     # TAG names should always be lowercase
---

[Download coursework PDF]({{site.url}}/assets/L46_project.pdf) \| [Source code on Github](https://github.com/semiluna/l46_project)

---
This is yet another final coursework for one of the [5 research modules](https://www.cl.cam.ac.uk/teaching/2223/L46/) I took during my Master's year at the University of Cambridge. 

[Knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation) (KD) is a popular technique used to compress large machine learning models into smaller ones that can be more easily deployed on devices with limited resources. 

A recent line of work investigates “student-friendly” teachers that are more appropriate for knowledge transfer; pruned networks constitute one such example, with recent results suggesting that students trained with pruned models outperform models trained with unpruned teachers. While this line of work looks promising for future deployment of performant networks on resource-limited environments, little work has been done on investigating whether knowledge distillation using pruned networks can be used for Graph Neural Networks (GNNs). 

This project investigates whether pruned GNNs are _better teachers_ than their unpruned counterparts. 

The **key takeaway** is that pruning GAT networks improves the distillation process on transductive node classficaition tasks. This is promising for many memory-constrained applications that require GNNs to perform inference, such as mobile navigation services and privacy-driven local recommender systems on apps.