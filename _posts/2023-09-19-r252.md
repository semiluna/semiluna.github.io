---
title: Towards an even better understanding of sharpness-aware minimisation
date: 2023-09-19 14:33:00 +0000
# categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [master's, theory of deep learning, deep learning, optimisation, SAM]     # TAG names should always be lowercase
---

[Download the coursework PDF]({{site.url}}/assets/R252_report.pdf) \| [Source code on Github]({https://github.com/Kausta/R252_SAM/tree/sharpness})

![r252-sharpness]({{site.url}}/assets/R252.png)
_Geometric visualisation of the minima of 4 different optimisers and configurations._

--- 

This is yet another final coursework I submitted for one of [the 5 research modules](https://www.cl.cam.ac.uk/teaching/2223/R252/) I took in my Master's year at Cambridge.[^1]

The course focused on the Theory of Deep Learning, and the research project involved investigating a curious property of [Sharpness-Aware Minimisation](https://medium.com/@m0nadsblog/sharpness-aware-minimization-fcea0fd766d8) (SAM), which is that its batched, stochastic version performs better than its full-batch counterpart. 

SAM is an optimiser that actively seeks out regions of the loss landscape that not only have low values, but also are "less sharp". This is a very desirable properly that training minimma should have, since many researchers believe that this flatter minima correlates with better generalisation power. 

The **key takeaway** of our project is that batched SAM finds more asymmetric minima, _and more asymmetric minima correlates with higher test-time performance_. It's important to note that just because a minima is asymmetric does not mean it is "less flat" or "better" than a possibly more symmetric minima. But it is hypothesised that [asymmetric minima are more common than symmetric ones](https://proceedings.neurips.cc/paper_files/paper/2019/hash/01d8bae291b1e4724443375634ccfa0e-Abstract.html), so it is possible that a batched version of SAM simply has an easier time finding a desirable minima. 

An alternative explanation could also be related to the fact that stochasticity is hypothesised to be an [implicit regulariser](https://arxiv.org/abs/2101.12176) in SGD, and therefore may very well prove to be a regulariser for SAM.

---

[^1]: This work was a joint project with my coursemates Kian Cross, Halfdan Holm, and Caner Korkmaz. 


