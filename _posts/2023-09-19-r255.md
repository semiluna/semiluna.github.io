---
title: Undesirable communication behaviour in cooperative agents with decentralised critics
date: 2023-09-19 14:33:00 +0000
# categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [master's, reinforcement learning, marl, rl, gnns]     # TAG names should always be lowercase
---

[Download coursework PDF]({{site.url}}/assets/R255_report.pdf) \| [Source code on Github](https://github.com/semiluna/self_interested_comms)

---

![r255-image]({{site.url}}/assets/R255.png)
_The architectural overview of the agents used in the project._

This is yet another final coursework for one of the [5 research modules](https://www.cl.cam.ac.uk/teaching/2223/L45) I took during my Master's year at the University of Cambridge. 

In [multi-agent reinforcement learning](https://en.wikipedia.org/wiki/Multi-agent_reinforcement_learning) (MARL), agents in a shared environment learn to make decisions based on local observations. Within MARL, a line of research focuses on allowing agents _to communicate_ with one another, for example by [adding Graph Neural Networks to their architectures](https://arxiv.org/abs/2008.02616). 

The effects of GNNs as communication channels have only been studied in the _decentralised actor - centralised critic framework_; that is, all cooperative agents **share the same model parameters**. However, many real-world problems can be more readily modelled as individual agents with local critics that learn to cooperate in a shared environment; network routers from different ISPs and self-driving cars from different companies are two such examples. In such scenarios, it is unrealistic to assume to presence of a centralised critic, but may be beneficial to allow such agents to communicate to improve the overall performance of the system. Therefore, in this project, **I investigate how communication affects learning in cooperative MARL agents that have decentralised critics**; that is, each agent has its own model paramters. 

The **key takeaway** is that introducing communication does **not** lead to a performance increase, with a post-hoc analysis showing that these agents may **learn to lie to each other**. Communication seems to not help to stabilise learning; it could also be the case that the reward function is not be good enough to enforce cooperative behaviours in these types of decentralised agents. 




